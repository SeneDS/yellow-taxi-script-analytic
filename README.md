## Business Questions

These questions focus on trends, seasonality, and long-term insights, emphasizing why analyzing multiple months of data is crucial.  

### **Market Demand & Seasonality**  
1. **How does the demand for yellow taxis fluctuate over time (daily, weekly, monthly, and seasonally)?**  
2. **What are the peak hours for yellow taxi trips in different boroughs and zones?**  
3. **How do weather conditions or major events (holidays, sports events) impact yellow taxi usage over time?**  

### **Customer Behavior & Ride Characteristics**  
4. **What are the most popular pickup and drop-off locations, and how do they change over time?**  
5. **What is the average trip distance, and how does it vary by borough, time of day, and season?**  
6. **How many trips have only one passenger versus multiple passengers, and does this change seasonally?**  

### **Financial & Pricing Analysis**  
7. **How does the total fare revenue of yellow taxis change over time?**  
8. **What is the average fare per trip, and how does it vary by borough, time of day, and trip distance?**  
9. **What is the proportion of different payment types (credit card, cash, etc.), and has it changed over time?**  
10. **How often do passengers tip, and what factors (time of day, borough, fare amount) influence tip amounts?**  
11. **How much revenue is generated from additional charges (MTA tax, congestion surcharge, airport fees), and has it changed over time?**  

### **Competitive Insights & Operational Efficiency**  
12. **Which boroughs or zones have the highest and lowest trip volumes, and how do they compare over time?**  
13. **How frequently do yellow taxis serve airports (JFK, LaGuardia, Newark), and what is the average fare for these trips?**  
14. **How often do taxis use different rate codes (e.g., standard rate vs. negotiated fares), and how do these rates vary across boroughs?**  
15. **How long do trips typically take, and is there a trend of increasing or decreasing trip durations over time?**  

These insights will help you understand market demand, customer behavior, and financial trends, providing a solid foundation for competing against yellow taxis in NYC.

# Vid√©o pr√©sentation du Cours (une sorte de page de garde)

# Section 0 : Bienvenue dans ce cours

## Vid√©o : Introduction - Pr√©sentation du Sc√©nario

## Notes importantes pour les √©tudiant(e)s (texte)

- Ressources : Comment les t√©l√©charger et les utiliser

- D√©tail des Questions Business

# Section 1 : Fondamentaux de l'Ing√©nierie des Donn√©es (Data Engineering)

N.B : Il s'agit uniquement des fondamentaux de l'ing√©nierie des donn√©es √† conna√Ætre pour pouvoir bien suivre ce Projet. En aucun cas, je ne pr√©tends vous donner ici toutes les notions fondamentales de la Data Engineering. Si le sujet vous int√©resse, vous trouverez beeaucoup d'autres ressources adapt√©es sur Internet.

## Texte : Comprendre le Concept de Data Warehouse

Dans le monde des donn√©es, un des d√©fis majeurs est de d√©placer les donn√©es depuis leurs sources vers d'autres environnements afin qu'elles puissent √™tre exploit√©es pour g√©n√©rer de l'information utile aux d√©cisions strat√©giques. C'est ici qu'intervient le **Data Warehouse**.  

#### **Origine et Objectif du Data Warehouse**  
Les Data Warehouses ont √©t√© d√©velopp√©s dans les ann√©es 1980 pour r√©pondre √† un besoin croissant : transformer les donn√©es issues des syst√®mes op√©rationnels en un outil puissant d'aide √† la d√©cision. Le principe cl√© d‚Äôun Data Warehouse est de **rassembler les donn√©es provenant de sources multiples en un seul endroit**, puis de les **transformer et structurer** afin qu‚Äôelles puissent √™tre stock√©es et exploit√©es efficacement.  

#### **Les Probl√®mes de Silos de Donn√©es**  
Dans les grandes organisations, il est tr√®s courant de voir appara√Ætre des **silos de donn√©es**. Chaque d√©partement (Finance, Marketing, Ressources Humaines, etc.) travaille souvent avec ses propres syst√®mes et bases de donn√©es, car ils ont des objectifs et des priorit√©s diff√©rentes. Cela entra√Æne une dispersion des donn√©es et rend difficile leur exploitation pour une analyse globale et coh√©rente.  

#### **Pourquoi un Data Warehouse ?**  
Afin de maximiser la valeur des donn√©es, il est essentiel de **les centraliser** dans un **espace unique**, un Data Warehouse. Cela permet de :  
‚úÖ **Briser les silos de donn√©es** et faciliter leur acc√®s.  
‚úÖ **Fournir une vue unifi√©e** des donn√©es √† travers toute l‚Äôorganisation.  
‚úÖ **Am√©liorer la qualit√© des analyses et la prise de d√©cision** gr√¢ce √† une structuration optimis√©e des donn√©es.  

#### **Comment fonctionne un Data Warehouse ?**  
Les solutions de Data Warehouse sont con√ßues pour stocker et traiter de grandes quantit√©s de donn√©es de mani√®re efficace. Les utilisateurs peuvent acc√©der aux donn√©es via des **requ√™tes SQL**, qui permettent d‚Äôinterroger les tables selon une structure bien d√©finie. Cela facilite l‚Äôanalyse et la production de rapports d√©taill√©s √† partir des donn√©es centralis√©es.  

#### **Le Data Warehouse dans ce cours**  
Dans ce cours, nous allons utiliser **Google BigQuery** comme Data Warehouse. BigQuery est une solution cloud qui offre de puissantes capacit√©s de stockage et d‚Äôanalyse de donn√©es √† grande √©chelle. Nous explorerons en d√©tail son fonctionnement et ses avantages dans une le√ßon d√©di√©e.  

üìå **R√©capitulatif : Pourquoi utiliser un Data Warehouse ?**  
‚úÖ Centralisation des donn√©es pour une meilleure accessibilit√©.  
‚úÖ Am√©lioration des performances d‚Äôanalyse et de requ√™tage.  
‚úÖ Suppression des silos pour une vision globale des donn√©es.  
‚úÖ Facilit√© d‚Äôutilisation gr√¢ce au langage SQL.  

## Vid√©o : Diff√©rence entre ETL et ELT

### **D√©finition de l'ETL** 
**Extract ‚Äì Transform ‚Äì Load**  
1. **Extract** : Extraction des donn√©es depuis plusieurs sources.  
2. **Transform** : Transformation avant le stockage (nettoyage, agr√©gation, jointures).  
3. **Load** : Chargement des donn√©es transform√©es dans l'entrep√¥t de donn√©es (Data Warehouse).  

---

### **D√©finition de l'ELT**  
**Extract ‚Äì Load ‚Äì Transform**  
1. **Extract** : Extraction des donn√©es brutes depuis plusieurs sources.  
2. **Load** : Chargement des donn√©es brutes directement dans un Data Warehouse ou Data Lake.  
3. **Transform** : Transformation directement dans le stockage (via SQL, Spark, BigQuery, Snowflake).  

---

### **Diff√©rences cl√©s entre ETL et ELT**  
| Crit√®re         | ETL                          | ELT                          |
|----------------|-----------------------------|-----------------------------|
| **Quand ?**     | Avant le stockage           | Apr√®s le stockage           |
| **O√π ?**       | Serveur ETL                  | Data Warehouse / Data Lake  |
| **Performance** | Plus lent (serveurs limit√©s) | Plus rapide (scalabilit√© du cloud) |
| **Flexibilit√©** | Structur√©                    | Adapt√© au Big Data          |
| **Stockage**   | Seulement les donn√©es utiles | Toutes les donn√©es brutes   |

---

### **Conclusion**  
- L‚Äô**ETL** est adapt√© aux syst√®mes traditionnels n√©cessitant des donn√©es transform√©es avant stockage.  
- L‚Äô**ELT** est plus efficace pour les architectures cloud, exploitant la puissance de calcul des entrep√¥ts modernes.  
- Le choix d√©pend du contexte, de l‚Äôinfrastructure et du volume des donn√©es.  


# Section 2 : Commencez avec Google Cloud Platform (GCP)

## Cr√©ation d'un compte GCP

Le processus d'inscription √† GCP est tr√®s facile. Il suffit de suivre ces √©tapes :

- Acc√©dez √† GCP console (nterface graphique de GCP) en allant sur https://console.cloud.google.com/ via n'importe quel navigateur web

- Connectez-vous avec votre compte Google (par exemple, Gmail).

- Inscrivez-vous √† GCP en utilisant votre compte Google.

N.B : Lors de l'inscription, il vous sera demand√© de renseigner un moyen de paiement, par exemple une carte bancaire. Mais rasurez-vous, rien ne vous sera factur√© √† ce stade.

De plus, si c'est otre premi√®re inscription, vous aurez droit √† un cr√©dit de 300$ √† consommer en 90 jours (3 mois). Avec ce cr√©dit, vous pouvez essayer n'importe quel service de GCP dans les 90 jours.

## Exploration de GCP console et cr√©ation d'un Projet

- Cr√©er un Projet

- Epingler les services suivants pour pouvoir vite y acc√©der : IAM & Admin, Cloud Storage, BigQuery, Composer, Vertex AI, Looker Studio. Nous n'utiliserons pas les deux derniers dans cette formation mais j'en parlerai bri√®vement.

- Utilisation de Cloud Shell et Cloud Editor : Tapez par exemple *ls* dans Cloud Shell et ex√©cutez le cote *print("Hello world") dans Cloud Editor avec python3 hello_world.py

## Pr√©sentation des services GCP que nous utiliserons dans ce cours

Les Services du domaine "Analytics" :

- Cloud Composer: Un service g√©r√© (*Managed Service*) pour Airflow. Airflow est un outil d'orchestration de t√¢ches bas√© sur Python.

- Dataproc: Un service g√©r√© pour Hadoop qui inclut HDFS, MapReduce, Spark, Presto, et bien plus encore. (On ne l'utilisera pas particuli√®rement dans cette formation mais c'est un important outil pour un Data Engineer travaillant dans GCP)

- Looker Studio: Un outil simple de visualisation pour visualiser les donn√©es. A ne pas confondre avec Looker qui est un outil BI complet pour visualiser les donn√©es dans des rapports et des tableaux de bord.

- BigQuery: Un service d'entrep√¥t de donn√©es sans serveur (*A serverless data warehouse service*)

- etc.

Les Services du domaine "Storage" :

- Cloud Storage : Un stockage d'objets sans serveur pour stocker des fichiers volumineux. Il est souvent utilis√© comme un Data Lake.

- etc.

Les Services du domaine "Management"

- IAM & Admin: Gestion des utilisateurs et des projets pour tous les services GCP

- etc.

Les Services du domaine "Artificial Intelligence"

- Vertex AI : comporte tous les outils dont vous avez besoin pour cr√©er du machine learning et du MLOps ‚Äì par exemple, des notebooks, des pipelines, un magasin de mod√®les et d'autres services li√©s au machine learning




# Section 3 : Extraction des donn√©es depuis la source vers Google Cloud Storage

## Vid√©o : Clonage du r√©pertoire GitHub et installation des d√©pendances dans Cloud Shell

Bienvenue dans cette le√ßon o√π il s'agira de :

- cloner le r√©pertoire github du projet dans le Cloud Shell
- Cr√©er et activer un environnement virtuel Python
- d'installer les packages n√©cessaires.

## Vid√©o : Cr√©ation d‚Äôun Bucket GCS et upload du projet

Bienvenue dans cette le√ßon o√π il s'agira de :

- Cr√©er un Bucket GCS (Console GCP)

- Uploader le r√©pertoire du projet dans ce Bucket (Cloud Shell)

***export DESTINATION_BUCKET_NAME=yellow-taxi-trips-analytics-data-bucket***

***gcloud storage cp -r nyc-yellow-taxi-trips/* gs://$DESTINATION_BUCKET_NAME/from-git/***


## Video : Exploration des donn√©es source et r√©flexion sur l'automatisation du t√©l√©chargement des fichiers parquet

## Video : Limites du script simple et am√©liorations dans le Script optimis√© pour un Pipeline en production

## Vid√©o : Impl√©mentation et ex√©cution du script optimis√© pour l'extraction des donn√©es vers GCS

## Le√ßon Texte : **Conclusion : Extraction des donn√©es vers Google Cloud Storage**

### **Slide 1 : Titre & Introduction**
- **Titre : Fin de la phase d‚ÄôExtraction - Passage √† l‚Äô√©tape suivante**
- **Sous-titre : Centralisation des donn√©es dans BigQuery**
- Image d‚Äôillustration : Pipeline de donn√©es avec une fl√®che vers BigQuery

---

### **Slide 2 : R√©capitulatif de la phase d‚ÄôExtraction**
‚úÖ **Nous avons r√©alis√© :**
- **Clonage du r√©pertoire GitHub et installation des d√©pendances**
- **Cr√©ation d‚Äôun Bucket GCS et upload du projet**
- **Exploration des donn√©es source et r√©flexion sur leur automatisation**
- **Identification des limites d‚Äôun script simple**
- **Impl√©mentation et ex√©cution d‚Äôun script optimis√© pour l‚Äôextraction des fichiers PARQUET vers GCS**

---

### **Slide 3 : Importance de cette √©tape**
üìå **Pourquoi cette phase est cruciale ?**
- Permet de **collecter les donn√©es brutes** dans un espace de stockage centralis√© (GCS).
- Assure un **pipeline robuste et automatis√©** pour l‚Äôingestion des fichiers.
- Pose les bases pour l‚Äô√©tape suivante : **le chargement des donn√©es dans BigQuery.**

---

### **Slide 4 : Prochaine √©tape - Centralisation des donn√©es dans BigQuery**
üöÄ **Phase suivante : Le chargement des donn√©es dans BigQuery (Load)**
- Objectif : **Transf√©rer les donn√©es brutes de GCS vers BigQuery** pour les rendre exploitables.
- Permet d‚Äôavoir **une seule source de v√©rit√©** pour toutes les analyses.
- D√©ploiement d‚Äôun processus **scalable et optimis√© pour le Cloud.**

üìå **Ce que nous allons voir :**
‚úÖ Cr√©ation des tables BigQuery
‚úÖ Chargement des fichiers PARQUET depuis GCS
‚úÖ Automatisation du processus avec Cloud Composer

---

### **Slide 5 : Conclusion & Transition**
üéØ **F√©licitations !** Vous avez termin√© la phase d‚Äôextraction des donn√©es.

üì¢ **Dans la prochaine section, nous verrons comment charger ces donn√©es dans BigQuery pour centraliser l‚Äôinformation et faciliter les analyses.**

üîú **Passons maintenant √† la phase Load !** üöÄ


# Section 4 : Chargement des donn√©es brutes de GCS vers BigQuery pour les rendre exploitables



# Section 5 : Data Analysis and Visualization (Data Analyst/Business Analyst)

# Section 6 : Building Machine Learning Models with BigQueryML

## Qu'est-ce que BigQuery ML ?

## D√©mo : Construire et √©valuer des Mod√®les ML pour pr√©dire le prix d'un trajet en taxi jaune √† New Yoork
